---
title: "Predictive Models"
output: html_document
---

### 1. Data Preparation
```{r}
# load the data
df1 <- read.csv('df1.csv')
df2 <- read.csv('df2.csv')
```
The workflow involves data splitting. The data will be divided into 3 parts, including training set (50%), validation set (25%) & testing set (25%). Each set is having its purposes as follows:

  - **Training set:** it is used to build models. Additionally, Hyperparameter tuning is also carried out
  - **Validation set:** it is used to choose the best cutoff/threshold which will differentiate the good credit from the bad one
  - **Testing set:** it is used to evaluate the model's performance. 

<br/> 
**JUSTIFICATION** The reason for creating a validation set in the workflow is to avoid an over-optimistic scenario when it comes to choose threshold value on the testing set. Besides, the common concept of default threshold at 50% will not yield the best results from the model. In addition, comparing model performance at a common threshold will be meaningless (Fawcett, 2005)^[Fawcett, T. (2006) An Introduction to ROC Analysis.Pattern Recognition Letters, 27, 861-874.
https://doi.org/10.1016/j.patrec.2005.10.010] 
<br/>
<br/>
The metrics area under the curve (AUC) is mainly used in this report to choose the model. Unlike accuaracy, specificity and sensitivity, it has a property of staying constant under various threshold values. Moreover, the AUC also indicates the model's power of how good the classifier can differentiate the binary outcomes. 

```{r message=FALSE, warning=FALSE}
# split training set and test set
# instruction: https://cran.r-project.org/web/packages/caTools/caTools.pdf 
# install.packages('caTools')
library(caTools)
set.seed(696)
split1 = sample.split(df1$credit, SplitRatio = 0.5)
training <- subset(df1, split1 == TRUE)
not_training <- subset(df1, split1 == FALSE)

split2 <- sample.split(not_training$credit, SplitRatio = 0.5)
testing <- subset(not_training, split2 == TRUE)
validation <- subset(not_training, split2 == FALSE)
```

```{r}
# number of observations in training set
nrow(training)     
```

```{r}
# number of observations in training set
nrow(validation)    
```

```{r}
# number of observations in test set: 160
nrow(testing)       
```

### 2. Decision Tree
```{r message=FALSE, warning=FALSE}
# instruction: https://cran.r-project.org/web/packages/caret/caret.pdf 
# install.packages('caret')
library(caret)
library(rpart)
```

##### 2.1 hyperparameter tuning
```{r}
# setting conditions for trainControl
fitControl <- trainControl(method = "cv",   # set up cross validation
                           ## 5-fold cross validation
                           number = 5,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using the following function which
                           ## computes sensitivity, specificity & area under the ROC
                           summaryFunction = twoClassSummary)
```

```{r}
# search for parameter(s) for tuning random forest
modelLookup('rpart2')
```

```{r}
# setting different values of maxdepth for the model 
man_grid <-  expand.grid(maxdepth = c(1:20))
# doing the grid search                                                              
set.seed(696)
tree_model1 <- train(credit ~.-id, 
                     data = training, 
                     method = "rpart2", 
                     ## Specify which metric to optimize, by default, this is the accuracy
                     metric = "ROC",
                     trControl = fitControl,
                     tuneGrid = man_grid)
```

```{r}
tree_model1
```

```{r}
plot(tree_model1)
```
At the maxdepth of 4, tree model will yield the best area under the curve (AUC)

##### 2.2 building the model
```{r}
# build the model by manually selecting the parameter after the grid search
# setting conditions for trainControl
fitControl <- trainControl(method = "cv",   # set up cross validation
                           ## 5-fold cross validation
                           number = 5,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using the following function which
                           ## computes sensitivity, specificity & area under the ROC
                           summaryFunction = twoClassSummary)
```

```{r}
# choosing maxdepth = 4 as the best tree model is given by doing the grid search above
hyperparams <- expand.grid(maxdepth = 4)
```

```{r}
# building the model
set.seed(696)
tree_model2 <- train(credit ~.-id, 
                     data = training, 
                     method = "rpart2", 
                     tuneGrid = hyperparams,
                     metric = "ROC",
                     trControl = fitControl)
```

```{r}
plot(varImp(tree_model2))
```

```{r}
rpart.plot::rpart.plot(tree_model2$finalModel)
```

##### 2.3 using validation set to choose the best threshold
```{r}
# extracting class posterior probabilities
prob <- predict(tree_model2, newdata = validation, type = 'prob')
# creating a vector of probability from tree prediction on the validation set
tree_val_prob <- prob[[2]]
```

```{r message=FALSE, warning=FALSE}
# choosing the best therold for the classifer
library(ROCR)
threshold_pred <- prediction(tree_val_prob, validation$credit)
evaluation <- performance(threshold_pred, 'acc')
# Identify the best threshold 
max <- which.max(evaluation@y.values[[1]])
val_accuracy <- evaluation@y.values[[1]][max]
tree_threshold <- evaluation@x.values[[1]][max]
# reduce the threshold value a small degree so that the model could cover the true positives better
tree_threshold <- tree_threshold - 0.001
```

```{r}
plot(evaluation, main = 'Threshold vs Accuracy', xlab = 'Threshold', ylab = 'Accuracy')
abline(h = val_accuracy, col = 'red')
abline(v = tree_threshold, col = 'red')
```

```{r}
# the threshold value for tree model
tree_threshold
```

##### 2.4 test the model on testing set
```{r}
# extracting class posterior probabilities
prob <- predict(tree_model2, newdata = testing, type = 'prob')
# creating a vector of probability from tree prediction on the validation set
tree_test_prob <- prob[[2]]
tree_test_pred <- as.factor(ifelse(prob[[2]] >= tree_threshold, "Good", "Bad"))
```

```{r}
# accuracy of the model on the testing set = 78.75%
confusionMatrix(testing$credit, tree_test_pred, positive = 'Good')
```

```{r}
tree_test_acc <- confusionMatrix(testing$credit, tree_test_pred, positive = 'Good')[[3]][[1]]
tree_test_sens <- confusionMatrix(testing$credit, tree_test_pred, positive = 'Good')[[4]][[1]]
tree_test_spec <- confusionMatrix(testing$credit, tree_test_pred, positive = 'Good')[[4]][[2]]

```

##### 2.5 making predictions on df2, i.e. scoring sheet
```{r}
df2$tree_prob <- predict(tree_model2, newdata = df2, type = 'prob')[[2]]
df2$tree_pred <- as.factor(ifelse(df2$tree_prob >= tree_threshold, "Good", "Bad"))
df2[,c('id','tree_pred')]
```

### 3. Random Forest

```{r message=FALSE, warning=FALSE}
# install.packages('rpart')
library(rpart)
```

##### 3.1 hyperparameter tuning

```{r}
# setting conditions for trainControl
fitControl <- trainControl(method = "cv",   # set up cross validation
                           ## 3-fold cross validation
                           number = 5,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using the following function which
                           ## computes sensitivity, specificity & area under the ROC
                           summaryFunction = twoClassSummary)
```

```{r}
# search for parameter(s) for tuning random forest
modelLookup('rf')
```

```{r}
# setting different values of mtry for the model 
man_grid <-  expand.grid(mtry = c(1:13))
```

```{r}
# doing the grid search                                                              
set.seed(696)
# building the model                        NB. This takes some time to run           
forest_model1 <- train(credit ~.-id, 
                       data = training, 
                       method = "rf", 
                       ## Specify which metric to optimize, by default, this is the accuracy
                       metric = "ROC",
                       trControl = fitControl,
                       verbose = FALSE,
                       tuneGrid = man_grid)
```

```{r}
forest_model1 
```

```{r}
plot(forest_model1, xlab = 'mtry')
```
The final value used for the model was mtry = 3. 
```{r}
forest_model1$bestTune    # mtry = 3
```

##### 3.2 building the model
```{r}
# build the model by selecting the mtry given by the grid searches (mtry = 3)
hyperparams <- expand.grid(mtry=3)
fitControl <- trainControl(method = "cv",   # set up cross validation
                           ## 5-fold cross validation
                           number = 5,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using the following function which
                           ## computes sensitivity, specificity & area under the ROC
                           summaryFunction = twoClassSummary)
set.seed(696)
forest_model2 <- train(credit ~.-id, 
                       data = training, 
                       method = 'rf', 
                       tuneGrid = hyperparams,
                       metric = "ROC",
                       trControl = fitControl,
                       verbose = F)
forest_model2
```

```{r}
forest_var <- varImp(forest_model2)
plot(forest_var)
```

##### 3.3 using validation set to choose the best threshold
```{r}
# extracting class posterior probabilities
prob <- predict(forest_model2, newdata = validation, type = 'prob')
# creating a vector of probability from tree prediction on the validation set
forest_val_prob <- prob[[2]]

# choosing the best therold for the classifer
library(ROCR)
threshold_pred <- prediction(forest_val_prob, validation$credit)
evaluation <- performance(threshold_pred, 'acc')
# Identify the best threshold 
max <- which.max(evaluation@y.values[[1]])
val_accuracy <- evaluation@y.values[[1]][max]
forest_threshold <- evaluation@x.values[[1]][max]
# reduce the threshold value a small degree so that the model could cover the true positives better
forest_threshold <- forest_threshold - 0.001
```

```{r}
plot(evaluation, main = 'Threshold vs Accuracy', xlab = 'Threshold', ylab = 'Accuracy')
abline(h = val_accuracy, col = 'red')
abline(v = forest_threshold, col = 'red')
```

```{r}
forest_threshold
```

##### 3.4 test the model on testing set
```{r}
# extracting class posterior probabilities
prob <- predict(forest_model2, newdata = testing, type = 'prob')
# creating a vector of probability from tree prediction on the validation set
forest_test_prob <- prob[[2]]
forest_test_pred <- as.factor(ifelse(prob[[2]] >= forest_threshold, "Good", "Bad"))
# accuracy level of the model on the testing set = 64%
confusionMatrix(testing$credit, forest_test_pred, positive = 'Good')
```

```{r}
forest_test_acc <- confusionMatrix(testing$credit, forest_test_pred, positive = 'Good')[[3]][[1]]
forest_test_sens <- confusionMatrix(testing$credit, forest_test_pred, positive = 'Good')[[4]][[1]]
forest_test_spec <- confusionMatrix(testing$credit, forest_test_pred, positive = 'Good')[[4]][[2]]

```

##### 3.5 making predictions on df2, i.e. scoring sheet
```{r}
df2$forest_prob <- predict(forest_model2, newdata = df2, type = 'prob')[[2]]
df2$forest_pred <- as.factor(ifelse(df2$forest_prob >= forest_threshold, "Good", "Bad"))
df2[,c('id','tree_pred', 'forest_pred')]
```

### 4. Gradient Boosting

```{r message=FALSE, warning=FALSE}
library(gbm)
```

##### 4.1 hyperparameter tuning 

**grid search 1**

```{r message=FALSE, warning=FALSE}
# setting conditions for trainControl
fitControl <- trainControl(method = "repeatedcv",   # set up cross validation
                           ## 3-fold cross validation
                           number = 3,
                           ## repeat cross validation 1 time
                           repeats = 1,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using the following function which
                           ## computes sensitivity, specificity & area under the ROC
                           summaryFunction = twoClassSummary)
# setting different values for gbm's parameters
man_grid <-  expand.grid(n.trees = c(100,200,500,800,1000,1200,1500),
                         shrinkage = c(0.05, 0.01,0.1),
                         interaction.depth = c(1,3,5,8,10),
                         n.minobsinnode = c(5,10,15,20))

# doing the grid search                  NB. THIS TAKE AROUND 15 MINUTES - SKIP THIS AND GO TO SECTION
set.seed(696)
gbm_model1 <- train(credit ~.-id,
                    data = training,
                    method = "gbm",
                    ## Specify which metric to optimize, by default, this is the accuracy
                    metric = "ROC",
                    trControl = fitControl,
                    verbose = FALSE,
                    tuneGrid = man_grid)
```


```{r}
plot(gbm_model1)
```

```{r}
gbm_model1$bestTune # n.trees = 100, interaction.depth = 3, shrinkage = 0.05, n.minobsinnode = 5
```

**grid search 2**

```{r message=FALSE, warning=FALSE}
# setting conditions for trainControl
fitControl <- trainControl(method = "repeatedcv",   # set up cross validation
                           ## 3-fold cross validation
                           number = 3,
                           ## repeat cross validation 1 time
                           repeats = 1,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using the following function which
                           ## computes sensitivity, specificity & area under the ROC
                           summaryFunction = twoClassSummary)
# setting different values for gbm's parameters
man_grid <-  expand.grid(n.trees = c(20,50,80,100,120,150,180,200),
                         shrinkage = c(0.05, 0.01,0.1),
                         interaction.depth = c(1,2,3,4,5),
                         n.minobsinnode = c(1,2,3,4,5,6,7))

# doing the grid search                  NB. THIS TAKE AROUND 15 MINUTES - SKIP THIS AND GO TO SECTION d.2.2
set.seed(696)
gbm_model2 <- train(credit ~.-id,
                    data = training,
                    method = "gbm",
                    ## Specify which metric to optimize, by default, this is the accuracy
                    metric = "ROC",
                    trControl = fitControl,
                    verbose = FALSE,
                    tuneGrid = man_grid)
```

```{r}
plot(gbm_model2)
```

```{r}
gbm_model2$bestTune # n.trees = 20, interaction.depth = 4, shrinkage = 0.01, n.minobsinnode = 5
```

##### 4.2 building the model
```{r warning=FALSE}
# build the model by manually selecting the parameters after the grid searches
hyperparams <- expand.grid(n.trees = 20,
                           interaction.depth = 4, 
                           shrinkage = 0.01,
                           n.minobsinnode = 5)
fitControl <- trainControl(method = "repeatedcv",   # set up cross validation
                           ## 5-fold cross validation
                           number = 5,
                           ## repeat cross validation 5 times
                           repeats = 5,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using the following function which
                           ## computes sensitivity, specificity & area under the ROC
                           summaryFunction = twoClassSummary,
                           savePredictions = "all")
set.seed(696)
gbm_model8 <- train(credit ~.-id, 
                    data = training, 
                    method = "gbm", 
                    tuneGrid = hyperparams,
                    metric = "ROC",
                    trControl = fitControl,
                    verbose = F)

gbm_model8 
```

```{r}
gbm_var <- varImp(gbm_model8)
plot(gbm_var)
```

##### 4.3 using validation set to choose the best threshold
```{r}
# extracting class posterior probabilities
prob <- predict(gbm_model8, newdata = validation, type = 'prob')
# creating a vector of probability from tree prediction on the validation set
forest_val_prob <- prob[[2]]

# choosing the best therold for the classifer
library(ROCR)
threshold_pred <- prediction(forest_val_prob, validation$credit)
evaluation <- performance(threshold_pred, 'acc')
# Identify the best threshold 
max <- which.max(evaluation@y.values[[1]])
val_accuracy <- evaluation@y.values[[1]][max]
gbm_threshold <- evaluation@x.values[[1]][max]
# reduce the threshold value a small degree so that the model could cover the true positives better
gbm_threshold <- gbm_threshold - 0.001
```

```{r}
plot(evaluation, main = 'Threshold vs Accuracy in GBM model', xlab = 'Threshold', ylab = 'Accuracy')
abline(h = val_accuracy, col = 'red')
abline(v = gbm_threshold, col = 'red')
```

```{r}
gbm_threshold
```

##### 4.4 test the model on testing set
```{r}
# extracting class posterior probabilities
prob <- predict(gbm_model8, newdata = testing, type = 'prob')
# creating a vector of probability from tree prediction on the validation set
gbm_test_prob <- prob[[2]]
gbm_test_pred <- as.factor(ifelse(prob[[2]] >= gbm_threshold, "Good", "Bad"))
# accuracy level of the model on the testing set = 77%
confusionMatrix(testing$credit, gbm_test_pred, positive = 'Good')
```

```{r}
gbm_test_acc <- confusionMatrix(testing$credit, gbm_test_pred, positive = 'Good')[[3]][[1]]
gbm_test_sens <- confusionMatrix(testing$credit, gbm_test_pred, positive = 'Good')[[4]][[1]]
gbm_test_spec <- confusionMatrix(testing$credit, gbm_test_pred, positive = 'Good')[[4]][[2]]
```

##### 4.5 making predictions on df2, i.e. scoring sheet
```{r}
df2$gbm_prob <- predict(gbm_model8, newdata = df2, type = 'prob')[[2]]
df2$gbm_pred <- as.factor(ifelse(df2$gbm_prob >= gbm_threshold, "Good", "Bad"))
df2[,c('id','tree_pred', 'forest_pred', 'gbm_pred')]
```

### 5. Evaluating models' performances

```{r}
# calculating the area under the ROC
prob_list <- list(tree_test_prob, forest_test_prob, gbm_test_prob)
auc <- c()
for(i in 1:3){
  t <- prediction(prob_list[[i]], testing$credit)
  v <- performance(t, "auc")
  auc <- c(auc, v@y.values[[1]])
}
# creating a vector of models' accuracy
acc <- c(tree_test_acc, forest_test_acc, gbm_test_acc)
# creating a vector of models' sensitivity
sens <- c(tree_test_sens, forest_test_sens, gbm_test_sens)
# creating a vector of models' specificity
spec <- c(tree_test_spec, forest_test_spec, gbm_test_spec)
# creating a matrix of metrics evaluating models' performaces
model_performance <- cbind(auc, acc, sens, spec)
rownames(model_performance) <- c('tree', 'forest', 'gbm')
model_performance
```

```{r}
df2[,c('id', 'tree_pred', 'forest_pred', 'gbm_pred')]
```

**CONCLUSION 9** Generally speaking, all three models have the same predictions in which id 782 & 783 have the good credit - the same results from dataframe df1. However, random forest seems to be the worst model which is not commonly worse than dececion tree. The reason for this could be that random forest select some “Features” randomly to build the Trees, if a “Feature” is important, sometimes Random Forest will build trees that will not have the significance that the“Feature” has in the final decision. In another word, it is the choice of mtry in this case. The best short-term model could be decision tree which is easily explainable to non-tech audience. However, gradient boosting has a slightly better area under the curve. It indicates the model's power of distinguishing the binary outcomes efficiently. Moreover, to build a gradient boosting model, a grid search seems to be a must. If more time were invested for that process, better gradient boosting models could be used. 
<br/>
<br/>
Importantly, considering the sensitivity and specificity of the model is very crucial. It's not just about to choose the model having the best accuracy. It's about the business and their choice to have appropriate strategy associated with the decisions of whether or not a person is having a good credit. To think about this, further research is needed to consider the real business cost of the decision behind choosing appropriate threshold. Whether business owner needs to have more customers to give loans away or they have to be more selective/conservative on the criteria of choosing. Because sensitivity and specificity are all about that. Therefore, there is no absolute answer for choosing the right threshold. Contexts should be taken into consideration carefully. 
<br/>
<br/>
To explain the decision tree process to non-tech audience, it's a series of question for their customers. It begins with customers' history of their account. Then follow-up questions are used to determine whether those customers are qualified for the loan or not. This process is visualised easily by showing up the tree model to non-tech non-tech executives. 
```{r}
rpart.plot::rpart.plot(tree_model2$finalModel)
```

### 6. Next Step
```{r}
# create a dataframe containing prediction probabilities 
# given by tree, forest & gbm models on the test set
# the dataframe also includes numeric column of the credit (1 for Good & 0 for Bad)
numeric_credit_testing <- as.numeric(testing$credit) - 1
models_prob_predictions <- data.frame(tree_test_prob, 
                                      forest_test_prob,
                                      gbm_test_prob, 
                                      numeric_credit_testing)
head(models_prob_predictions)
```

```{r}
# # write a csv file so that the following question, i.e. h, can be refered to 
# write.csv(models_prob_predictions, 'models_prob_predictions.csv', row.names = FALSE)
```


